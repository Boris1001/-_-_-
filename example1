!pip install -q pymorphy2 # установка лемматизатора pymorphy2

from google.colab import files, drive

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import time
#import pymorphy2

from tensorflow.keras import utils                                                                                       # Для работы с категориальными данными
from tensorflow.keras.models import Sequential                                                                           # Полносвязная модель
from tensorflow.keras.optimizers import Adam                                                                             # Оптимизатор
from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation # Слои для сети
from tensorflow.keras.preprocessing.text import Tokenizer                                                                # Токенайзер
from tensorflow.keras.preprocessing.sequence import pad_sequences                                                        # Метод для работы с последовательностями

from sklearn.preprocessing import LabelEncoder                                                                           # Для кодирования тестовых лейблов
from sklearn.model_selection import train_test_split                                                                     # Для разделения выборки на тестовую и обучающую

%matplotlib inline

path = '/content/drive/My Drive/Базы/Тексты писателей/' # Путь к директории на диске, где находятся изображения
drive.mount('/content/drive')

!rm -R '/content/texts' # Проверяем наличие папки texts. Если есть, удаляем ее

!unzip -q '/content/drive/My Drive/Базы/Тексты писателей.zip' -d '/content/texts'  # загружаем базу

# Функция для чтения файла

def readText(filename):
  with open(filename, 'r') as file:
    text = file.read()
    text = text.replace('\n', ' ')
    return text
    
# Конвертируем исходный текст в лист слов с начальной формой

def text2Words(text):
  morph = pymorphy2.MorphAnalyzer()                                             # Инициализируем инструмент для работы с морфемами и более
  words = text.split(' ')
  result = [morph.parse(word)[0].normal_form for word in words]                 # Превращаем каждое слово в элемент списка
  return result
  
# Переменные для имён и количества классов

className = ['О. Генри', 'Стругацкие', 'Булгаков', 'Саймак', 'Фрай', 'Брэдберри']
nClasses = len(className)

# Переменные для загрузки текстов

trainText = []                                                                  # Для обучающих текстов
testText = []                                                                   # Для тестовых текстов

trainTextNorm = []                                                              # Для обучающих нормализованных текстов
testTextNorm = []                                                               # Для тестовых нормализованных текстов

# Функция получения текстов

def getTrainTestText():
  for i in className:                                                           # В каждом классе          
    for j in os.listdir('/content/texts/Тексты писателей/'):                    # В каждом файле в созданной ранее папке
      if i in j:                                                                # Если название класса есть в файле
        if 'Обучающая' in j:                                                    # Для обучающей выборки
          trainTextNorm.append(readText('/content/texts/Тексты писателей/' + j))    # Добавляем в обучающую выборку
          print(j, 'добавлен в обучающую выборку')
        elif 'Тестовая' in j:                                                   # Для тестовой выборки
          testTextNorm.append(readText('/content/texts/Тексты писателей/' + j))     # Добавляем в тестовую выборку
          print(j, 'добавлен в тестовую выборку')
    print()
    
getTrainTestText()

print('В обучающей выборке: ')
for i in range(len(trainText)):
     print(f'В {i + 1} тексте {len(trainText[i])} символов.') 
print()
#print('В обучающей нормализованной  выборке: ')
for i in range(len(trainTextNorm)):
     print(f'В {i + 1} тексте {len(trainTextNorm[i])} символов.')   # Количество символов в каждом тексте
     
 maxWordsCount = 20000 # Определяем максимальное количество слов/индексов, учитываемое при обучении текстов

# Воспользуемся встроенной в Keras функцией Tokenizer для разбиения текста и превращения в матрицу числовых значений
# num_words=maxWordsCount - определяем максимальное количество слов/индексов, учитываемое при обучении текстов
# filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n' - избавляемся от ненужных символов
# lower=True - приводим слова к нижнему регистру
# split=' ' - разделяем слова по пробелу
# char_level=False - токенизируем по словам (Если будет True - каждый символ будет рассматриваться как отдельный токен )

tokenizer = Tokenizer(num_words=maxWordsCount, filters='!"#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff', lower=True, split=' ', oov_token='unknown', char_level=False)

# Переводим тексты в словари частотности

#tokenizer.fit_on_texts(trainText)
tokenizer.fit_on_texts(trainTextNorm)

# Смотрим что получилось

items = list(tokenizer.word_index.items())                                      # Вытаскиваем индексы слов для просмотра
print(items[:50])                                                               # Посмотрим 50 самых часто встречающихся слов
print("Размер словаря", len(items)) 

trainWordIndexes = tokenizer.texts_to_sequences(trainTextNorm)                  # Обучающие тексты в индексы
testWordIndexes = tokenizer.texts_to_sequences(testTextNorm)                    # Тестовые тексты в индексы

# функция разделения выборки на вектора

def getSetFromIndexes(wordIndexes, xLen, step):
  xSample = []                                                                  
  wordsLen = len(wordIndexes)
  index = 0

  while (index + xLen) <= wordsLen:
    xSample.append(wordIndexes[index: index + xLen])                            # Добавляем срезы длиной xLen
    index += step

  return xSample
  
 
 def createSetsMultiClasses(wordIndexes, xLen, step):
  nClasses = len(wordIndexes)
  classesXSamples = []                                                          # Здесь будет список размером "кол-во классов * кол-во окон в тексте * длину окна (например: 6*1341*1000)"
  for wI in wordIndexes:                                                        # Для каждого текста
    classesXSamples.append(getSetFromIndexes(wI, xLen, step))                   # Добавляем разбитую на вектора выборку

  # Формируем один общий xSamples
  xSamples = []                                                                 # Здесь будет список размером "суммарное кол-во окон во всех текстах*длину окна (например, 15779*1000)"
  ySamples = []                                                                 # Здесь будет список размером "суммарное кол-во окон во всех текстах*вектор длиной 6"

  for i in range(nClasses):                                                     # Для каждого класса
    xT = classesXSamples[i]
    for j in range(len(xT)):                                                    # Для каждого слова
      xSamples.append(xT[j])                                                    # Добавляем в общий список выборки
      ySamples.append(utils.to_categorical(i, nClasses))                        # Добавляем метку класса в формате OHE (правильный ответ)

  xSamples = np.array(xSamples)
  ySamples = np.array(ySamples)
 
  return (xSamples, ySamples)


# Задаём базовые параметры
xLen = 1000                                                                     # Длина отрезка текста, по которой анализируем, в словах
step = 100    


xTrain, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)           # Извлекаем обучающие выборки
xTest, yTest = createSetsMultiClasses(testWordIndexes, xLen,  step)             # Извлекаем тестовые выборки

# Преобразовываем полученные выборки из последовательности индексов в матрицы нулей и единиц по принципу Bag of Words

xTrain01 = tokenizer.sequences_to_matrix(xTrain.tolist())                      
xTest01 = tokenizer.sequences_to_matrix(xTest.tolist())  

# Представляем тестовую выборку в удобных для распознавания размерах
def createTestMultiClasses(wordIndexes, xLen, step): #функция принимает последовательность индексов, размер окна, шаг окна

  #Для каждого из 6 классов
  #Создаём тестовую выборку из индексов
  nClasses = len(wordIndexes) #Задаем количество классов
  xTest6Classes01 = []               #Здесь будет список из всех классов, каждый размером "кол-во окон в тексте * 20000 (при maxWordsCount=20000)"
  xTest6Classes = []                 #Здесь будет список массивов, каждый размером "кол-во окон в тексте * длину окна"(6 по 420*1000)
  for wI in wordIndexes:                       #Для каждого тестового текста из последовательности индексов
    sample = (getSetFromIndexes(wI, xLen, step)) #Тестовая выборка размером "кол-во окон*длину окна"(например, 420*1000)
    xTest6Classes.append(sample)              # Добавляем в список
    xTest6Classes01.append(tokenizer.sequences_to_matrix(sample)) #Трансформируется в Bag of Words в виде "кол-во окон в тексте * 20000"
  xTest6Classes01 = np.array(xTest6Classes01)                     #И добавляется к нашему списку, 
  xTest6Classes = np.array(xTest6Classes)                     #И добавляется к нашему списку, 
  
  return xTest6Classes01, xTest6Classes  #функция вернёт тестовые данные: TestBag 6 классов на n*20000 и xTestEm 6 по n*1000

# Распознаём тестовую выборку и выводим результаты
def recognizeMultiClass(model, xTest, modelName):
  print("НЕЙРОНКА: ", modelName)
  print()
  
  totalSumRec = 0 # Сумма всех правильных ответов
  
  #Проходим по всем классам
  for i in range(nClasses):
    #Получаем результаты распознавания класса по блокам слов длины xLen
    currPred = model.predict(xTest[i])
    #Определяем номер распознанного класса для каждохо блока слов длины xLen
    currOut = np.argmax(currPred, axis=1)

    evVal = []
    for j in range(nClasses):
      evVal.append(len(currOut[currOut==j])/len(xTest[i]))

    totalSumRec += len(currOut[currOut==i])
    recognizedClass = np.argmax(evVal) #Определяем, какой класс в итоге за какой был распознан
    
    #Выводим результаты распознавания по текущему классу
    isRecognized = "Это НЕПРАВИЛЬНЫЙ ответ!"
    if (recognizedClass == i):
      isRecognized = "Это ПРАВИЛЬНЫЙ ответ!"
    str1 = 'Класс: ' + className[i] + " " * (11 - len(className[i])) + str(int(100*evVal[i])) + "% сеть отнесла к классу " + className[recognizedClass]
    print(str1, " " * (55-len(str1)), isRecognized, sep='')
  
  #Выводим средний процент распознавания по всем классам вместе
  print()
  sumCount = 0
  for i in range(nClasses):
    sumCount += len(xTest[i])
  print("Средний процент распознавания ", int(100*totalSumRec/sumCount), "%", sep='')

  print()
  # Создаём полносвязную сеть
model02 = Sequential()
# Первый полносвязный слой
model02.add(Dense(200, input_dim=maxWordsCount, activation="relu"))
# Слой регуляризации Dropout
model02.add(Dropout(0.25))
# Слой пакетной нормализации
model02.add(BatchNormalization())
# Второй полносвязный слой
model02.add(Dense(800, input_dim=maxWordsCount, activation="relu"))
# Слой регуляризации Dropout
model02.add(Dropout(0.3))
# Слой пакетной нормализации
model02.add(BatchNormalization())
# Выходной полносвязный слой
model02.add(Dense(6, activation='softmax'))


model02.summary()  # выводим архитектуру модели

# График

plt.figure(figsize=(14,7))
plt.plot(history.history['accuracy'], 
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'], 
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

#Проверяем точность нейронки обученной на bag of words
pred = recognizeMultiClass(model02, xTest6Classes01, "Тексты 01 + Dense")


model_filename = path + 'weights/YOLO/YOLOv3__el1000__opt0.001__ep0_100.h5'     # Указываем имя файла для сохранения модели

# Сохраним веса модели в weights
model02.save_weights(model_filename)

# Сохраним значения ошибок в csv
loss = np.array(history.history['loss'])                                        # Получаем значения ошибки на обучающей выборке
val_loss = np.array(history.history['val_loss'])                                # Получаем значения ошибки на проверочной выборке

pd.DataFrame(loss).to_csv(path + '/csv/YOLO/loss__el1000_opt0.001__ep0_100.csv')
pd.DataFrame(loss).to_csv(path + 'csv/YOLO/valloss__el1000_opt0.001__ep0_100.csv')

model_weight =  path +'weights/YOLO/YOLOv3__el1000__opt0.001__ep0_100.h5'       # Веса модели

model02.load_weights('/content/drive/MyDrive/Базы/Тексты писателей/weights/YOLO/YOLOv3__el1000__opt0.001__ep0_100.h5', by_name = False, skip_mismatch = False) # Загружаем предобученные веса

# Дообучение

model02.compile(optimizer=Adam(0.0001), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

history2 = model02.fit(xTrain01, 
                      yTrain, 
                      epochs=10,
                      batch_size=256,
                      validation_data=(xTest01, yTest))

model02.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

#Обучаем сеть на выборке, сформированной по bag of words - xTrain01
history = model02.fit(xTrain01, 
                      yTrain, 
                      epochs=100,
                      batch_size=128,
                      validation_data=(xTest01, yTest))
